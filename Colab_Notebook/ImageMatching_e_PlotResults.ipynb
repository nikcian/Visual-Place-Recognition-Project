{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Ambiente e Repository"
      ],
      "metadata": {
        "id": "ukwlkoWFBM81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clonazione e Configurazione Branch 'Marco'"
      ],
      "metadata": {
        "id": "pq0XSXs-BSyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Reset Posizione Base\n",
        "os.chdir(\"/content\")\n",
        "project_path = \"/content/Visual-Place-Recognition-Project\"\n",
        "\n",
        "# Pulizia preventiva\n",
        "if os.path.exists(project_path):\n",
        "    print(f\"üßπ Pulizia vecchia cartella...\")\n",
        "    shutil.rmtree(project_path)\n",
        "\n",
        "# Clonazione Repository (Versione Nikcian)\n",
        "print(\"üì• Clonazione del repository (Nikcian)...\")\n",
        "subprocess.run([\"git\", \"clone\", \"https://github.com/nikcian/Visual-Place-Recognition-Project.git\"], check=True)\n",
        "\n",
        "if os.path.exists(project_path):\n",
        "    os.chdir(project_path)\n",
        "    print(f\"üìÇ Entrato in: {os.getcwd()}\")\n",
        "\n",
        "    print(\"üîÄ Passaggio al branch 'Marco'...\")\n",
        "    res = subprocess.run([\"git\", \"checkout\", \"Marco\"], capture_output=True)\n",
        "    if res.returncode != 0:\n",
        "        print(\"‚ö†Ô∏è Branch remoto non trovato, creo locale da main...\")\n",
        "        subprocess.run([\"git\", \"checkout\", \"-b\", \"Marco\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwArCiTxBSKo",
        "outputId": "e63a9b81-50af-4d2f-cea7-e9c47cdd5a61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Clonazione del repository (Nikcian)...\n",
            "üìÇ Entrato in: /content/Visual-Place-Recognition-Project\n",
            "üîÄ Passaggio al branch 'Marco'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installazione Dipendenze e Fix Librerie"
      ],
      "metadata": {
        "id": "zS9Bma1uBY1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX REQUIREMENTS\n",
        "req_path = os.path.join(\"VPR-methods-evaluation\", \"requirements.txt\")\n",
        "new_req_path = os.path.join(\"VPR-methods-evaluation\", \"requirements_colab.txt\")\n",
        "\n",
        "if os.path.exists(req_path):\n",
        "    print(f\"üîß Adattamento requirements per Colab...\")\n",
        "    with open(req_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    with open(new_req_path, \"w\") as f:\n",
        "        for line in lines:\n",
        "            pkg = line.split(\"==\")[0].split(\">=\")[0].strip()\n",
        "            if pkg.lower() in [\"torch\", \"torchvision\", \"numpy\", \"matplotlib\", \"scipy\", \"pillow\"]:\n",
        "                continue\n",
        "            if pkg: f.write(f\"{pkg}\\n\")\n",
        "\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", new_req_path, \"--quiet\"])\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"faiss-gpu\", \"gdown\", \"loguru\", \"einops\", \"--quiet\"])\n",
        "    print(\"üéâ Setup base completato!\")\n",
        "\n",
        "# Riparazione manuale e gestione FAISS\n",
        "print(\"‚öôÔ∏è Configurazione Faiss e utility...\")\n",
        "for pkg in [\"gdown\", \"loguru\", \"einops\"]:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"--quiet\"])\n",
        "\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"faiss-gpu\", \"--quiet\"])\n",
        "    print(\"‚úÖ Faiss-GPU installato.\")\n",
        "except:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"faiss-cpu\", \"--quiet\"])\n",
        "    print(\"‚úÖ Fallback: Faiss-CPU installato.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "ieHFJrqtBZ1R",
        "outputId": "d02bde12-002f-4c4a-d4c4-029d8224d92c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Adattamento requirements per Colab...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command '['/usr/bin/python3', '-m', 'pip', 'install', 'faiss-gpu', 'gdown', 'loguru', 'einops', '--quiet']' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2221136682.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"install\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_req_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--quiet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"install\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"faiss-gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gdown\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loguru\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"einops\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--quiet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üéâ Setup base completato!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/usr/bin/python3', '-m', 'pip', 'install', 'faiss-gpu', 'gdown', 'loguru', 'einops', '--quiet']' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"üîß Tentativo di riparazione manuale delle librerie...\")\n",
        "\n",
        "def install(package):\n",
        "    try:\n",
        "        print(f\"üì¶ Installazione {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
        "        print(f\"‚úÖ {package} installato!\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"‚ö†Ô∏è Errore installando {package}.\")\n",
        "        return False\n",
        "\n",
        "# 1. Installiamo le librerie accessorie sicure\n",
        "install(\"gdown\")\n",
        "install(\"loguru\")\n",
        "install(\"einops\")\n",
        "\n",
        "# 2. Gestione Intelligente di FAISS\n",
        "print(\"‚öôÔ∏è Configurazione Faiss...\")\n",
        "# Prima proviamo la versione GPU\n",
        "if not install(\"faiss-gpu\"):\n",
        "    print(\"üîÑ Fallback: Provo a installare faiss-cpu...\")\n",
        "    # Se fallisce, proviamo la CPU\n",
        "    if install(\"faiss-cpu\"):\n",
        "        print(\"‚úÖ Faiss-CPU installato con successo (modalit√† compatibile).\")\n",
        "    else:\n",
        "        print(\"‚ùå Errore critico: Impossibile installare Faiss (n√© GPU n√© CPU).\")\n",
        "else:\n",
        "    print(\"‚úÖ Faiss-GPU installato correttamente.\")\n",
        "\n",
        "print(\"\\nüéâ Riparazione completata. Ora puoi procedere al download dei dati.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZHwTeRLBkr4",
        "outputId": "5cd35f1c-ba38-4cfe-8c0a-33d3aead15fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Tentativo di riparazione manuale delle librerie...\n",
            "üì¶ Installazione gdown...\n",
            "‚úÖ gdown installato!\n",
            "üì¶ Installazione loguru...\n",
            "‚úÖ loguru installato!\n",
            "üì¶ Installazione einops...\n",
            "‚úÖ einops installato!\n",
            "‚öôÔ∏è Configurazione Faiss...\n",
            "üì¶ Installazione faiss-gpu...\n",
            "‚ö†Ô∏è Errore installando faiss-gpu.\n",
            "üîÑ Fallback: Provo a installare faiss-cpu...\n",
            "üì¶ Installazione faiss-cpu...\n",
            "‚úÖ faiss-cpu installato!\n",
            "‚úÖ Faiss-CPU installato con successo (modalit√† compatibile).\n",
            "\n",
            "üéâ Riparazione completata. Ora puoi procedere al download dei dati.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gestione Dataset e Modelli Esterni"
      ],
      "metadata": {
        "id": "DKV9KKCNBoCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset (SF, Tokyo, SVOX)"
      ],
      "metadata": {
        "id": "uyipafnNBqsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "import shutil\n",
        "\n",
        "os.chdir(project_path)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "datasets = {\n",
        "    \"sf_xs\": \"https://drive.google.com/file/d/1tQqEyt3go3vMh4fj_LZrRcahoTbzzH-y/view?usp=share_link\",\n",
        "    \"tokyo_xs\": \"https://drive.google.com/file/d/15QB3VNKj93027UAQWv7pzFQO1JDCdZj2/view?usp=share_link\",\n",
        "    \"svox\": \"https://drive.google.com/file/d/16iuk8voW65GaywNUQlWAbDt6HZzAJ_t9/view?usp=drive_link\"\n",
        "}\n",
        "\n",
        "for name, url in datasets.items():\n",
        "    if not os.path.exists(f\"data/{name}\"):\n",
        "        print(f\"‚¨áÔ∏è Scaricando {name}...\")\n",
        "        gdown.download(url, f\"data/{name}.zip\", quiet=False, fuzzy=True)\n",
        "        shutil.unpack_archive(f\"data/{name}.zip\", \"data\")\n",
        "        os.remove(f\"data/{name}.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whOXLvgSBqDS",
        "outputId": "bdbc9a86-eaee-4f69-c523-dbdf75f8bfd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Scaricando sf_xs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1tQqEyt3go3vMh4fj_LZrRcahoTbzzH-y\n",
            "From (redirected): https://drive.google.com/uc?id=1tQqEyt3go3vMh4fj_LZrRcahoTbzzH-y&confirm=t&uuid=d27335e4-08da-4252-867b-e6219726311e\n",
            "To: /content/Visual-Place-Recognition-Project/data/sf_xs.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.03G/1.03G [00:11<00:00, 89.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Scaricando tokyo_xs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15QB3VNKj93027UAQWv7pzFQO1JDCdZj2\n",
            "From (redirected): https://drive.google.com/uc?id=15QB3VNKj93027UAQWv7pzFQO1JDCdZj2&confirm=t&uuid=31387073-b94c-41c5-8eca-ad3e840190cf\n",
            "To: /content/Visual-Place-Recognition-Project/data/tokyo_xs.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141M/141M [00:01<00:00, 123MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Scaricando svox...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16iuk8voW65GaywNUQlWAbDt6HZzAJ_t9\n",
            "From (redirected): https://drive.google.com/uc?id=16iuk8voW65GaywNUQlWAbDt6HZzAJ_t9&confirm=t&uuid=8283d7ac-d9eb-45cc-b7a7-2fc77ce6d7d6\n",
            "To: /content/Visual-Place-Recognition-Project/data/svox.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.51G/3.51G [00:49<00:00, 70.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Image Matching Models"
      ],
      "metadata": {
        "id": "Ya9gvFxjB3Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix cartella esterna e sottomoduli\n",
        "target_folder = \"image-matching-models\"\n",
        "if os.path.exists(target_folder): shutil.rmtree(target_folder)\n",
        "subprocess.run([\"git\", \"clone\", \"--recursive\", \"https://github.com/alexstoken/image-matching-models.git\", target_folder], check=True)\n",
        "\n",
        "# Installazione modelli\n",
        "%cd {target_folder}\n",
        "!pip install -e . --quiet\n",
        "!pip install git+https://github.com/cvg/LightGlue.git yacs kornia kornia_moons py3_wget --quiet\n",
        "%cd ..\n",
        "\n",
        "# Configurazione PYTHONPATH per SuperGlue\n",
        "cwd = os.getcwd()\n",
        "immatch_path = f\"{cwd}/image-matching-models/matching/third_party/imatch-toolbox\"\n",
        "patch2pix_path = f\"{immatch_path}/third_party/patch2pix\"\n",
        "sys.path.insert(0, immatch_path)\n",
        "sys.path.insert(0, patch2pix_path)\n",
        "os.environ[\"PYTHONPATH\"] = f\"{immatch_path}:{patch2pix_path}:\" + os.environ.get(\"PYTHONPATH\", \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvSsdoiqB4Rh",
        "outputId": "a0f10d14-6a72-4eac-9652-19ff203d0525"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Visual-Place-Recognition-Project/image-matching-models\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.1/155.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m225.3/225.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m737.0/737.0 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for image-matching-models (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires fastapi<0.124.0,>=0.115.0, but you have fastapi 0.88.0 which is incompatible.\n",
            "google-adk 1.21.0 requires pydantic<3.0.0,>=2.0, but you have pydantic 1.10.26 which is incompatible.\n",
            "google-adk 1.21.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.22.0 which is incompatible.\n",
            "google-adk 1.21.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 11.0.3 which is incompatible.\n",
            "pydantic-settings 2.12.0 requires pydantic>=2.7.0, but you have pydantic 1.10.26 which is incompatible.\n",
            "langchain 1.2.0 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.26 which is incompatible.\n",
            "google-genai 1.55.0 requires pydantic<3.0.0,>=2.9.0, but you have pydantic 1.10.26 which is incompatible.\n",
            "google-genai 1.55.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "thinc 8.3.10 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.26 which is incompatible.\n",
            "albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.26 which is incompatible.\n",
            "gradio 5.50.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.88.0 which is incompatible.\n",
            "gradio 5.50.0 requires pydantic<=2.12.3,>=2.0, but you have pydantic 1.10.26 which is incompatible.\n",
            "gradio 5.50.0 requires starlette<1.0,>=0.40.0, but you have starlette 0.22.0 which is incompatible.\n",
            "mcp 1.24.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 1.10.26 which is incompatible.\n",
            "mcp 1.24.0 requires starlette>=0.27, but you have starlette 0.22.0 which is incompatible.\n",
            "langgraph 1.0.5 requires pydantic>=2.7.4, but you have pydantic 1.10.26 which is incompatible.\n",
            "sse-starlette 3.0.4 requires starlette>=0.49.1, but you have starlette 0.22.0 which is incompatible.\n",
            "langchain-core 1.2.1 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.26 which is incompatible.\n",
            "gradio-client 1.14.0 requires websockets<16.0,>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.12.2 which is incompatible.\n",
            "dataproc-spark-connect 1.0.1 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lightglue (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "/content/Visual-Place-Recognition-Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generazione File TXT (da preds.npy)"
      ],
      "metadata": {
        "id": "1MdZkMggCUIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recupero e Generazione per San Francisco XS e Tokyo XS"
      ],
      "metadata": {
        "id": "XZTIHIymCXft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Rimuove forzatamente la cartella se esiste e non √® montata bene\n",
        "if os.path.exists('/content/drive'):\n",
        "    try:\n",
        "        shutil.rmtree('/content/drive')\n",
        "    except:\n",
        "        print(\"Cartella drive occupata, procedo al mount forzato...\")\n",
        "\n",
        "# 2. Riesegui il mount (clicca sul link o autorizza il pop-up)\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 3. Verifica finale: ora dovresti vedere le tue cartelle\n",
        "print(\"\\n‚úÖ Cartelle trovate nel Drive:\")\n",
        "print(os.listdir('/content/drive/MyDrive/'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFGL5fggwcHT",
        "outputId": "10141e49-4ae1-4867-e2c3-b0da033d31e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "‚úÖ Cartelle trovate nel Drive:\n",
            "['Colab Notebooks', 'tokyo_xs.zip', 'VPR_Risultati_Finali', 'sf_xs.zip', 'svox.zip', 'VPR_Project_Final_Results']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "def process_dataset_to_txt(name, folder_on_drive):\n",
        "    print(f\"üîç Elaborazione {name}...\")\n",
        "    output_dir = f\"/content/Visual-Place-Recognition-Project/logs/netvlad_{name}/predictions_txt\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Percorso basato sulla tua struttura comunicata\n",
        "    drive_root = \"/content/drive/MyDrive/VPR_Risultati_Finali\"\n",
        "    target_path = os.path.join(drive_root, folder_on_drive, \"data/preds/preds.npy\")\n",
        "\n",
        "    if os.path.exists(target_path):\n",
        "        print(f\"‚úÖ TROVATO in: {target_path}\")\n",
        "        preds = np.load(target_path)\n",
        "    else:\n",
        "        # Fallback: ricerca ricorsiva se il percorso fisso fallisce\n",
        "        print(f\"‚ö†Ô∏è Percorso fisso non trovato. Avvio ricerca ricorsiva per {folder_on_drive}...\")\n",
        "        found_npy = next((p for p in Path(drive_root).rglob(\"preds.npy\")\n",
        "                         if folder_on_drive in str(p).lower()), None)\n",
        "        if not found_npy:\n",
        "            return print(f\"‚ùå ERRORE: preds.npy non trovato per {name} in {drive_root}\")\n",
        "        preds = np.load(found_npy)\n",
        "\n",
        "    # Percorsi immagini locali\n",
        "    db_folder = f\"/content/Visual-Place-Recognition-Project/data/{name}/test/database\"\n",
        "    q_folder = f\"/content/Visual-Place-Recognition-Project/data/{name}/test/queries\"\n",
        "\n",
        "    db_paths = sorted(glob.glob(f\"{db_folder}/*.jpg\") + glob.glob(f\"{db_folder}/*.png\"))\n",
        "    q_paths = sorted(glob.glob(f\"{q_folder}/*.jpg\") + glob.glob(f\"{q_folder}/*.png\"))\n",
        "\n",
        "    for i, row in enumerate(preds):\n",
        "        if i >= len(q_paths): break\n",
        "        out_file = os.path.join(output_dir, f\"{i}.txt\")\n",
        "        with open(out_file, \"w\") as f:\n",
        "            f.write(f\"Query: {os.path.basename(q_paths[i])}\\n{os.path.abspath(q_paths[i])}\\nPredictions:\\n\")\n",
        "            for db_idx in row:\n",
        "                if db_idx < len(db_paths):\n",
        "                    f.write(f\"{os.path.abspath(db_paths[int(db_idx)])}\\n\")\n",
        "\n",
        "    print(f\"üéâ Creati {len(os.listdir(output_dir))} file TXT in: {output_dir}\")\n",
        "\n",
        "# Esecuzione con i nomi cartella corretti del tuo Drive\n",
        "process_dataset_to_txt(\"sf_xs\", \"netvlad_sfxs\")\n",
        "process_dataset_to_txt(\"tokyo_xs\", \"netvlad_tokyo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRSXJwdbCWJz",
        "outputId": "51d05db9-e79e-484c-dbb4-7ccdb3fabfb4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Elaborazione sf_xs...\n",
            "‚ö†Ô∏è Percorso fisso non trovato. Avvio ricerca ricorsiva per netvlad_sfxs...\n",
            "üéâ Creati 1000 file TXT in: /content/Visual-Place-Recognition-Project/logs/netvlad_sf_xs/predictions_txt\n",
            "üîç Elaborazione tokyo_xs...\n",
            "‚ö†Ô∏è Percorso fisso non trovato. Avvio ricerca ricorsiva per netvlad_tokyo...\n",
            "üéâ Creati 315 file TXT in: /content/Visual-Place-Recognition-Project/logs/netvlad_tokyo_xs/predictions_txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generazione per SVOX (Sun & Night)"
      ],
      "metadata": {
        "id": "uzGozH5mCdGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_svox_txt(condition):\n",
        "    name = f\"svox_{condition}\"\n",
        "    folder_on_drive = f\"netvlad_svox_{condition}\"\n",
        "    print(f\"üîç Elaborazione {name.upper()}...\")\n",
        "\n",
        "    output_dir = f\"/content/Visual-Place-Recognition-Project/logs/{folder_on_drive}/predictions_txt\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    drive_root = \"/content/drive/MyDrive/VPR_Risultati_Finali\"\n",
        "    # Struttura specifica per SVOX: netvlad_svox_sun -> data -> preds -> preds.npy\n",
        "    target_path = os.path.join(drive_root, folder_on_drive, \"data/preds/preds.npy\")\n",
        "\n",
        "    if os.path.exists(target_path):\n",
        "        preds = np.load(target_path)\n",
        "    else:\n",
        "        found_npy = next((p for p in Path(drive_root).rglob(\"preds.npy\")\n",
        "                         if folder_on_drive in str(p).lower()), None)\n",
        "        if not found_npy: return print(f\"‚ùå ERRORE: Preds non trovato per {name}\")\n",
        "        preds = np.load(found_npy)\n",
        "\n",
        "    # Immagini SVOX\n",
        "    q_path_obj = list(Path(\"/content/Visual-Place-Recognition-Project/data/svox\").rglob(f\"queries_{condition}\"))\n",
        "    if not q_path_obj: return print(f\"‚ùå ERRORE: Immagini SVOX {condition} non trovate.\")\n",
        "\n",
        "    q_folder = str(q_path_obj[0])\n",
        "    db_folder = os.path.join(os.path.dirname(q_folder), \"gallery\")\n",
        "\n",
        "    db_paths = sorted(glob.glob(f\"{db_folder}/*.jpg\") + glob.glob(f\"{db_folder}/*.png\"))\n",
        "    q_paths = sorted(glob.glob(f\"{q_folder}/*.jpg\") + glob.glob(f\"{q_folder}/*.png\"))\n",
        "\n",
        "    for i, row in enumerate(preds):\n",
        "        if i >= len(q_paths): break\n",
        "        out_file = os.path.join(output_dir, f\"{i}.txt\")\n",
        "        with open(out_file, \"w\") as f:\n",
        "            f.write(f\"Query: {os.path.basename(q_paths[i])}\\n{os.path.abspath(q_paths[i])}\\nPredictions:\\n\")\n",
        "            for db_idx in row:\n",
        "                if db_idx < len(db_paths):\n",
        "                    f.write(f\"{os.path.abspath(db_paths[int(db_idx)])}\\n\")\n",
        "    print(f\"‚úÖ TXT Generati per {name}\")\n",
        "\n",
        "generate_svox_txt(\"sun\")\n",
        "generate_svox_txt(\"night\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28X1AG89CfL4",
        "outputId": "b8e2b46f-73bd-43db-a60b-e0bc67d4e22b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Elaborazione SVOX_SUN...\n",
            "‚úÖ TXT Generati per svox_sun\n",
            "üîç Elaborazione SVOX_NIGHT...\n",
            "‚úÖ TXT Generati per svox_night\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Esecuzione Matching"
      ],
      "metadata": {
        "id": "NPwjCviyFk9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sf_xs"
      ],
      "metadata": {
        "id": "-HEGrGiaFoJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# --- 1. CONFIGURAZIONE PERCORSI ---\n",
        "dataset_name = \"sf_xs\"\n",
        "matchers = [\"superpoint-lg\", \"loftr\", \"superglue\"]\n",
        "project_root = \"/content/Visual-Place-Recognition-Project\"\n",
        "%cd {project_root}\n",
        "\n",
        "# Cartelle su Drive per il salvataggio permanente\n",
        "drive_base = f\"/content/drive/MyDrive/VPR_Project_Final_Results/{dataset_name}\"\n",
        "csv_drive_dir = f\"{drive_base}/CSVs\"\n",
        "torch_drive_dir = f\"{drive_base}/torch_files\"\n",
        "\n",
        "os.makedirs(csv_drive_dir, exist_ok=True)\n",
        "os.makedirs(torch_drive_dir, exist_ok=True)\n",
        "\n",
        "# --- 2. FUNZIONE ESTRAZIONE CSV (Adattata al tuo codice) ---\n",
        "def save_inliers_csv(preds_dir, matcher_name):\n",
        "    print(f\"üìÑ Generazione CSV per {matcher_name}...\")\n",
        "    inliers_dir = Path(f\"{preds_dir}_{matcher_name}\")\n",
        "    data_list = []\n",
        "\n",
        "    # Cerchiamo tutti i .torch generati\n",
        "    torch_files = sorted(list(inliers_dir.glob(\"*.torch\")), key=lambda x: int(x.stem))\n",
        "\n",
        "    for t_file in torch_files:\n",
        "        # Carichiamo la lista di risultati (uno per ogni candidato top-20)\n",
        "        results = torch.load(t_file, weights_only=False)\n",
        "        # Prendiamo il numero di inliers del PRIMO candidato (quello che determina R@1)\n",
        "        # o salviamo la media/max se preferisci per le estensioni 6.1/6.2\n",
        "        num_inliers_top1 = results[0]['num_inliers'] if len(results) > 0 else 0\n",
        "\n",
        "        data_list.append({\n",
        "            'query_id': t_file.stem,\n",
        "            'num_inliers': num_inliers_top1\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    csv_path = f\"{csv_drive_dir}/{matcher_name}_inliers.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"‚úÖ CSV salvato: {csv_path}\")\n",
        "\n",
        "# --- 3. ESECUZIONE BATCH ---\n",
        "for matcher in matchers:\n",
        "    print(f\"\\n{'-'*50}\\nüî• MATCHING CORRENTE: {matcher.upper()}\\n{'-'*50}\")\n",
        "\n",
        "    preds_dir = f\"logs/netvlad_{dataset_name}/predictions_txt\"\n",
        "    inliers_local_dir = f\"{preds_dir}_{matcher}\"\n",
        "\n",
        "    # A. Esecuzione Matching\n",
        "    !python match_queries_preds.py \\\n",
        "        --preds-dir \"$preds_dir\" \\\n",
        "        --matcher \"$matcher\" \\\n",
        "        --device cuda \\\n",
        "        --im-size 512 \\\n",
        "        --num-preds 20\n",
        "\n",
        "    # B. Backup dei .torch su Drive (Necessario per non perderli)\n",
        "    print(f\"üì¶ Backup file .torch su Drive...\")\n",
        "    dest_torch = f\"{torch_drive_dir}/{matcher}\"\n",
        "    if os.path.exists(dest_torch): shutil.rmtree(dest_torch)\n",
        "    shutil.copytree(inliers_local_dir, dest_torch)\n",
        "\n",
        "    # C. Generazione CSV per Estensioni\n",
        "    save_inliers_csv(preds_dir, matcher)\n",
        "\n",
        "    # D. Esecuzione Reranking (Metrica Recall)\n",
        "    print(f\"üìà Calcolo Recall...\")\n",
        "    !python reranking.py \\\n",
        "        --preds-dir \"$preds_dir\" \\\n",
        "        --inliers-dir \"$inliers_local_dir\" \\\n",
        "        --num-preds 20\n",
        "\n",
        "print(f\"\\n{'-'*50}\\nüéâ PROCESSO COMPLETATO PER SF-XS\\n{'-'*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "MDY3bdM6Fms3",
        "outputId": "3935c52d-21be-4fa1-fa26-39f2cbfdfcb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/Visual-Place-Recognition-Project'\n",
            "/content\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: SUPERPOINT-LG\n",
            "--------------------------------------------------\n",
            "python3: can't open file '/content/match_queries_preds.py': [Errno 2] No such file or directory\n",
            "üì¶ Backup file .torch su Drive...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'logs/netvlad_sf_xs/predictions_txt_superpoint-lg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4177843864.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdest_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{torch_drive_dir}/{matcher}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minliers_local_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# C. Generazione CSV per Estensioni\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutil.copytree\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'logs/netvlad_sf_xs/predictions_txt_superpoint-lg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. CONFIGURAZIONE ---\n",
        "dataset_name = \"sf_xs\"\n",
        "# Riprendiamo LoFTR (che salter√† i file esistenti) e poi SuperGlue\n",
        "matchers = [\"loftr\", \"superglue\"]\n",
        "project_root = \"/content/Visual-Place-Recognition-Project\"\n",
        "%cd {project_root}\n",
        "\n",
        "# Percorsi Drive per salvataggio permanente\n",
        "drive_base = f\"/content/drive/MyDrive/VPR_Project_Final_Results/{dataset_name}\"\n",
        "csv_drive_dir = f\"{drive_base}/CSVs\"\n",
        "torch_drive_dir = f\"{drive_base}/torch_files\"\n",
        "\n",
        "os.makedirs(csv_drive_dir, exist_ok=True)\n",
        "os.makedirs(torch_drive_dir, exist_ok=True)\n",
        "\n",
        "# --- 2. FUNZIONE ESTRAZIONE CSV (Semplice, come la precedente) ---\n",
        "def save_inliers_csv_simple(preds_dir, matcher_name):\n",
        "    print(f\"üìÑ Generazione CSV temporaneo per {matcher_name}...\")\n",
        "    inliers_dir = Path(f\"{preds_dir}_{matcher_name}\")\n",
        "    data_list = []\n",
        "\n",
        "    torch_files = sorted(list(inliers_dir.glob(\"*.torch\")), key=lambda x: int(x.stem))\n",
        "    for t_file in torch_files:\n",
        "        results = torch.load(t_file, weights_only=False)\n",
        "        num_inliers_top1 = results[0]['num_inliers'] if len(results) > 0 else 0\n",
        "        data_list.append({'query_id': t_file.stem, 'num_inliers': num_inliers_top1})\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    csv_path = f\"{csv_drive_dir}/{matcher_name}_inliers.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"‚úÖ CSV salvato: {csv_path}\")\n",
        "\n",
        "# --- 3. ESECUZIONE BATCH ---\n",
        "for matcher in matchers:\n",
        "    print(f\"\\n{'-'*50}\\nüî• RIPRESA MATCHING: {matcher.upper()}\\n{'-'*50}\")\n",
        "\n",
        "    preds_dir = f\"logs/netvlad_{dataset_name}/predictions_txt\"\n",
        "    inliers_local_dir = f\"{preds_dir}_{matcher}\"\n",
        "\n",
        "    # A. Esecuzione Matching (Riprende dal punto di interruzione)\n",
        "    !python match_queries_preds.py \\\n",
        "        --preds-dir \"$preds_dir\" \\\n",
        "        --matcher \"$matcher\" \\\n",
        "        --device cuda \\\n",
        "        --im-size 512 \\\n",
        "        --num-preds 20\n",
        "\n",
        "    # B. Backup dei .torch su Drive (Fondamentale!)\n",
        "    print(f\"üì¶ Backup file .torch su Drive...\")\n",
        "    dest_torch = f\"{torch_drive_dir}/{matcher}\"\n",
        "    if os.path.exists(dest_torch): shutil.rmtree(dest_torch)\n",
        "    shutil.copytree(inliers_local_dir, dest_torch)\n",
        "\n",
        "    # C. CSV Semplice\n",
        "    save_inliers_csv_simple(preds_dir, matcher)\n",
        "\n",
        "    # D. Reranking Base\n",
        "    print(f\"üìà Calcolo Recall base...\")\n",
        "    !python reranking.py \\\n",
        "        --preds-dir \"$preds_dir\" \\\n",
        "        --inliers-dir \"$inliers_local_dir\" \\\n",
        "        --num-preds 20\n",
        "\n",
        "print(f\"\\nüéâ SF-XS COMPLETATO. I file .torch sono al sicuro su Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXUP72vwv2oH",
        "outputId": "e4802115-28ac-47f1-878f-eead8438b254"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Visual-Place-Recognition-Project\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• RIPRESA MATCHING: LOFTR\n",
            "--------------------------------------------------\n",
            "Downloading: \"http://cmp.felk.cvut.cz/~mishkdmy/models/loftr_outdoor.ckpt\" to /root/.cache/torch/hub/checkpoints/loftr_outdoor.ckpt\n",
            "100% 44.2M/44.2M [00:03<00:00, 13.9MB/s]\n",
            "100% 1000/1000 [1:02:51<00:00,  3.77s/it]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "üìÑ Generazione CSV temporaneo per loftr...\n",
            "‚úÖ CSV salvato: /content/drive/MyDrive/VPR_Project_Final_Results/sf_xs/CSVs/loftr_inliers.csv\n",
            "üìà Calcolo Recall base...\n",
            "100% 1000/1000 [00:01<00:00, 617.88it/s]\n",
            "R@1: 53.6, R@5: 55.3, R@10: 55.8, R@20: 56.2, R@100: 56.2\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• RIPRESA MATCHING: SUPERGLUE\n",
            "--------------------------------------------------\n",
            "/content/Visual-Place-Recognition-Project/image-matching-models/matching/third_party/imatch-toolbox/third_party/patch2pix/networks/ncn/model.py:233: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n",
            "  if checkpoint is not None and checkpoint is not '':\n",
            "/content/Visual-Place-Recognition-Project/image-matching-models/matching/third_party/imatch-toolbox/third_party/patch2pix/networks/ncn/model.py:264: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n",
            "  if checkpoint is not None and checkpoint is not '':\n",
            "Loaded SuperGlue model (\"outdoor\" weights)\n",
            "Loaded SuperPoint model\n",
            "Initialize SuperPoint_r4\n",
            "Initialize SuperGlue_r4\n",
            "100% 1000/1000 [24:05<00:00,  1.45s/it]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "üìÑ Generazione CSV temporaneo per superglue...\n",
            "‚úÖ CSV salvato: /content/drive/MyDrive/VPR_Project_Final_Results/sf_xs/CSVs/superglue_inliers.csv\n",
            "üìà Calcolo Recall base...\n",
            "100% 1000/1000 [00:01<00:00, 529.38it/s]\n",
            "R@1: 52.4, R@5: 55.3, R@10: 55.8, R@20: 56.2, R@100: 56.2\n",
            "\n",
            "üéâ SF-XS COMPLETATO. I file .torch sono al sicuro su Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generazione File CSV corretto"
      ],
      "metadata": {
        "id": "rg50kNoQtloV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from util import get_list_distances_from_preds\n",
        "\n",
        "# Configurazione (Assicurati che Drive sia montato)\n",
        "dataset_name = \"sf_xs\"\n",
        "matchers = [\"superpoint-lg\", \"loftr\", \"superglue\"]\n",
        "threshold = 25  # Soglia metri definita dal progetto\n",
        "\n",
        "# Percorsi coerenti con il tuo script batch\n",
        "drive_base = f\"/content/drive/MyDrive/VPR_Project_Final_Results/{dataset_name}\"\n",
        "csv_drive_dir = f\"{drive_base}/CSVs\"\n",
        "torch_drive_dir = f\"{drive_base}/torch_files\"\n",
        "preds_dir = f\"/content/Visual-Place-Recognition-Project/logs/netvlad_{dataset_name}/predictions_txt\"\n",
        "\n",
        "def enrich_csv_from_drive():\n",
        "    for matcher in matchers:\n",
        "        print(f\"\\nüìä Arricchimento dati per: {matcher.upper()}\")\n",
        "\n",
        "        # Percorso dei .torch su Drive (gi√† backuppati)\n",
        "        matcher_torch_path = Path(f\"{torch_drive_dir}/{matcher}\")\n",
        "\n",
        "        if not matcher_torch_path.exists():\n",
        "            print(f\"‚ö†Ô∏è Cartella torch non trovata su Drive per {matcher}. Salto...\")\n",
        "            continue\n",
        "\n",
        "        data_list = []\n",
        "        torch_files = sorted(list(matcher_torch_path.glob(\"*.torch\")), key=lambda x: int(x.stem))\n",
        "\n",
        "        for t_file in torch_files:\n",
        "            # 1. Carica distanze reali per determinare is_correct\n",
        "            txt_file = f\"{preds_dir}/{t_file.stem}.txt\"\n",
        "            # get_list_distances_from_preds restituisce le distanze GPS originali\n",
        "            geo_dists = get_list_distances_from_preds(txt_file)\n",
        "\n",
        "            # 2. Carica i risultati del matching\n",
        "            query_results = torch.load(t_file, weights_only=False)\n",
        "\n",
        "            # 3. Trova il massimo numero di inliers tra i 20 candidati (per ordinamento)\n",
        "            inliers_counts = [res['num_inliers'] for res in query_results]\n",
        "            max_val = max(inliers_counts) if inliers_counts else 0\n",
        "\n",
        "            # 4. Trova l'indice del migliore per determinare se √® corretto (R@1 reranked)\n",
        "            best_idx = inliers_counts.index(max_val) if inliers_counts else 0\n",
        "            is_correct = 1 if (geo_dists[best_idx] <= threshold) else 0\n",
        "\n",
        "            data_list.append({\n",
        "                'query_id': t_file.stem,\n",
        "                'max_inliers': max_val,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        # Generazione e sovrascrittura CSV\n",
        "        df = pd.DataFrame(data_list)\n",
        "        csv_path = f\"{csv_drive_dir}/{matcher}_inliers.csv\"\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"‚úÖ CSV Arricchito salvato: {csv_path}\")\n",
        "\n",
        "# Esecuzione\n",
        "enrich_csv_from_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIrHeW_Ato4x",
        "outputId": "15f0dbb2-6939-4fca-e560-c5e82cc0cee7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Arricchimento dati per: SUPERPOINT-LG\n",
            "‚úÖ CSV Arricchito salvato: /content/drive/MyDrive/VPR_Project_Final_Results/sf_xs/CSVs/superpoint-lg_inliers.csv\n",
            "\n",
            "üìä Arricchimento dati per: LOFTR\n",
            "‚úÖ CSV Arricchito salvato: /content/drive/MyDrive/VPR_Project_Final_Results/sf_xs/CSVs/loftr_inliers.csv\n",
            "\n",
            "üìä Arricchimento dati per: SUPERGLUE\n",
            "‚úÖ CSV Arricchito salvato: /content/drive/MyDrive/VPR_Project_Final_Results/sf_xs/CSVs/superglue_inliers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokyo_xs"
      ],
      "metadata": {
        "id": "9rDHlmtvGLZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# --- 1. CONFIGURAZIONE PERCORSI ---\n",
        "dataset_name = \"tokyo_xs\" # Cambiato in tokyo_xs\n",
        "matchers = [\"superpoint-lg\", \"loftr\", \"superglue\"]\n",
        "project_root = \"/content/Visual-Place-Recognition-Project\"\n",
        "%cd {project_root}\n",
        "\n",
        "# Cartelle su Drive per il salvataggio permanente (struttura dedicata a Tokyo)\n",
        "drive_base = f\"/content/drive/MyDrive/VPR_Project_Final_Results/{dataset_name}\"\n",
        "torch_drive_dir = f\"{drive_base}/torch_files\"\n",
        "\n",
        "os.makedirs(torch_drive_dir, exist_ok=True)\n",
        "\n",
        "# --- 2. ESECUZIONE BATCH TOKYO-XS ---\n",
        "for matcher in matchers:\n",
        "    print(f\"\\n{'-'*50}\\nüî• MATCHING CORRENTE: {matcher.upper()} (Dataset: {dataset_name})\\n{'-'*50}\")\n",
        "\n",
        "    preds_dir = f\"logs/netvlad_{dataset_name}/predictions_txt\"\n",
        "    inliers_local_dir = f\"{preds_dir}_{matcher}\"\n",
        "\n",
        "    # A. Esecuzione Matching (512x512, top 20 candidati)\n",
        "    # Lo script salta automaticamente se riprendi una sessione interrotta\n",
        "    !python match_queries_preds.py \\\n",
        "        --preds-dir \"$preds_dir\" \\\n",
        "        --matcher \"$matcher\" \\\n",
        "        --device cuda \\\n",
        "        --im-size 512 \\\n",
        "        --num-preds 20\n",
        "\n",
        "    # B. Backup dei .torch su Drive (Salvataggio permanente)\n",
        "    print(f\"üì¶ Backup file .torch su Drive...\")\n",
        "    dest_torch = f\"{torch_drive_dir}/{matcher}\"\n",
        "\n",
        "    # Se esiste gi√† una cartella parziale, la aggiorniamo\n",
        "    if os.path.exists(dest_torch):\n",
        "        shutil.rmtree(dest_torch)\n",
        "    shutil.copytree(inliers_local_dir, dest_torch)\n",
        "    print(f\"‚úÖ Backup completato in: {dest_torch}\")\n",
        "\n",
        "    # C. Esecuzione Reranking (Calcolo Recall base per Tabella 1)\n",
        "    print(f\"üìà Calcolo Recall base per {matcher}...\")\n",
        "    !python reranking.py \\\n",
        "        --preds-dir \"$preds_dir\" \\\n",
        "        --inliers-dir \"$inliers_local_dir\" \\\n",
        "        --num-preds 20\n",
        "\n",
        "print(f\"\\n{'-'*50}\\nüéâ PROCESSO COMPLETATO PER TOKYO-XS\\n{'-'*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSfSHN61GNiN",
        "outputId": "75a59b6d-7b24-4dc4-a3bb-574976425a6a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Visual-Place-Recognition-Project\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: SUPERPOINT-LG (Dataset: tokyo_xs)\n",
            "--------------------------------------------------\n",
            "100% 315/315 [00:00<00:00, 65846.29it/s]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "‚úÖ Backup completato in: /content/drive/MyDrive/VPR_Project_Final_Results/tokyo_xs/torch_files/superpoint-lg\n",
            "üìà Calcolo Recall base per superpoint-lg...\n",
            "100% 315/315 [00:01<00:00, 172.80it/s]\n",
            "R@1: 68.3, R@5: 72.1, R@10: 73.7, R@20: 78.7, R@100: 78.7\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: LOFTR (Dataset: tokyo_xs)\n",
            "--------------------------------------------------\n",
            "100% 315/315 [00:00<00:00, 66033.87it/s]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "‚úÖ Backup completato in: /content/drive/MyDrive/VPR_Project_Final_Results/tokyo_xs/torch_files/loftr\n",
            "üìà Calcolo Recall base per loftr...\n",
            "100% 315/315 [00:00<00:00, 479.31it/s]\n",
            "R@1: 68.3, R@5: 72.7, R@10: 73.7, R@20: 78.7, R@100: 78.7\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: SUPERGLUE (Dataset: tokyo_xs)\n",
            "--------------------------------------------------\n",
            "Loaded SuperGlue model (\"outdoor\" weights)\n",
            "Loaded SuperPoint model\n",
            "Initialize SuperPoint_r4\n",
            "Initialize SuperGlue_r4\n",
            "100% 315/315 [00:00<00:00, 66318.93it/s]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "‚úÖ Backup completato in: /content/drive/MyDrive/VPR_Project_Final_Results/tokyo_xs/torch_files/superglue\n",
            "üìà Calcolo Recall base per superglue...\n",
            "100% 315/315 [00:00<00:00, 432.17it/s]\n",
            "R@1: 69.5, R@5: 72.7, R@10: 74.9, R@20: 78.7, R@100: 78.7\n",
            "\n",
            "--------------------------------------------------\n",
            "üéâ PROCESSO COMPLETATO PER TOKYO-XS\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generazione File CSV corretto"
      ],
      "metadata": {
        "id": "oknuWqgMXmpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from util import get_list_distances_from_preds\n",
        "\n",
        "# --- 1. CONFIGURAZIONE ---\n",
        "# Puoi aggiungere altri nomi alla lista se hai gi√† finito anche SVOX\n",
        "datasets = [\"sf_xs\", \"tokyo_xs\"]\n",
        "matchers = [\"superpoint-lg\", \"loftr\", \"superglue\"]\n",
        "threshold = 25  # Soglia di 25 metri definita dal progetto\n",
        "\n",
        "def generate_final_csvs():\n",
        "    for dataset in datasets:\n",
        "        print(f\"\\n{'='*60}\\nüìä GENERAZIONE CSV PER DATASET: {dataset.upper()}\\n{'='*60}\")\n",
        "\n",
        "        # Percorsi sul Drive\n",
        "        drive_base = f\"/content/drive/MyDrive/VPR_Project_Final_Results/{dataset}\"\n",
        "        csv_drive_dir = f\"{drive_base}/CSVs\"\n",
        "        torch_drive_dir = f\"{drive_base}/torch_files\"\n",
        "\n",
        "        # Percorso dei TXT locali (necessari per le distanze GPS)\n",
        "        preds_txt_dir = f\"/content/Visual-Place-Recognition-Project/logs/netvlad_{dataset}/predictions_txt\"\n",
        "\n",
        "        os.makedirs(csv_drive_dir, exist_ok=True)\n",
        "\n",
        "        for matcher in matchers:\n",
        "            matcher_torch_path = Path(f\"{torch_drive_dir}/{matcher}\")\n",
        "\n",
        "            if not matcher_torch_path.exists():\n",
        "                print(f\"‚ö†Ô∏è Cartella torch non trovata per {matcher} in {dataset}. Salto...\")\n",
        "                continue\n",
        "\n",
        "            print(f\"üîÑ Elaborazione {matcher}...\")\n",
        "            data_list = []\n",
        "\n",
        "            # Prendiamo tutti i file .torch salvati su Drive\n",
        "            torch_files = sorted(list(matcher_torch_path.glob(\"*.torch\")), key=lambda x: int(x.stem))\n",
        "\n",
        "            for t_file in torch_files:\n",
        "                # 1. Recupero distanze reali dal file TXT locale\n",
        "                txt_file = f\"{preds_txt_dir}/{t_file.stem}.txt\"\n",
        "                if not os.path.exists(txt_file):\n",
        "                    continue\n",
        "                geo_dists = get_list_distances_from_preds(txt_file)\n",
        "\n",
        "                # 2. Carico i risultati del matching dal Drive\n",
        "                query_results = torch.load(t_file, weights_only=False)\n",
        "\n",
        "                # 3. Estraggo gli inliers per tutti i 20 candidati\n",
        "                inliers_counts = [res['num_inliers'] for res in query_results]\n",
        "\n",
        "                # 4. Calcolo il MASSIMO degli inliers (confidenza del miglior match)\n",
        "                max_val = max(inliers_counts) if inliers_counts else 0\n",
        "\n",
        "                # 5. Determino se il miglior match (quello con pi√π inliers) √® corretto\n",
        "                # Se pi√π immagini hanno lo stesso numero di inliers, prendiamo la prima (top-1)\n",
        "                best_idx = inliers_counts.index(max_val) if inliers_counts else 0\n",
        "                is_correct = 1 if (geo_dists[best_idx] <= threshold) else 0\n",
        "\n",
        "                data_list.append({\n",
        "                    'query_id': t_file.stem,\n",
        "                    'max_inliers': max_val,\n",
        "                    'is_correct': is_correct\n",
        "                })\n",
        "\n",
        "            # Salvataggio del CSV finale su Drive\n",
        "            df = pd.DataFrame(data_list)\n",
        "            csv_name = f\"{matcher}_stats_final.csv\"\n",
        "            csv_path = f\"{csv_drive_dir}/{csv_name}\"\n",
        "            df.to_csv(csv_path, index=False)\n",
        "            print(f\"‚úÖ CSV salvato: {csv_path} ({len(df)} query)\")\n",
        "\n",
        "# Esecuzione dello script\n",
        "generate_final_csvs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtpCZ1VDGxsO",
        "outputId": "f9d4bd31-31cd-4b79-de17-3cd7f1a7a70d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìä GENERAZIONE CSV PER DATASET: SF_XS\n",
            "============================================================\n",
            "üîÑ Elaborazione superpoint-lg...\n",
            "‚úÖ CSV salvato: /content/drive/MyDrive/VPR_Project_Final_Results/sf_xs/CSVs/superpoint-lg_stats_final.csv (1000 query)\n",
            "üîÑ Elaborazione loftr...\n",
            "‚úÖ CSV salvato: /content/drive/MyDrive/VPR_Project_Final_Results/sf_xs/CSVs/loftr_stats_final.csv (1000 query)\n",
            "üîÑ Elaborazione superglue...\n",
            "‚úÖ CSV salvato: /content/drive/MyDrive/VPR_Project_Final_Results/sf_xs/CSVs/superglue_stats_final.csv (1000 query)\n",
            "\n",
            "============================================================\n",
            "üìä GENERAZIONE CSV PER DATASET: TOKYO_XS\n",
            "============================================================\n",
            "üîÑ Elaborazione superpoint-lg...\n",
            "‚úÖ CSV salvato: /content/drive/MyDrive/VPR_Project_Final_Results/tokyo_xs/CSVs/superpoint-lg_stats_final.csv (315 query)\n",
            "üîÑ Elaborazione loftr...\n",
            "‚úÖ CSV salvato: /content/drive/MyDrive/VPR_Project_Final_Results/tokyo_xs/CSVs/loftr_stats_final.csv (315 query)\n",
            "üîÑ Elaborazione superglue...\n",
            "‚úÖ CSV salvato: /content/drive/MyDrive/VPR_Project_Final_Results/tokyo_xs/CSVs/superglue_stats_final.csv (315 query)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## svox (sun e night)"
      ],
      "metadata": {
        "id": "BgYA1aVzXpll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# --- 1. CONFIGURAZIONE ---\n",
        "# Gestiamo entrambi i set di SVOX in un colpo solo\n",
        "svox_datasets = [\"svox_sun\", \"svox_night\"]\n",
        "matchers = [\"superpoint-lg\", \"loftr\", \"superglue\"]\n",
        "project_root = \"/content/Visual-Place-Recognition-Project\"\n",
        "%cd {project_root}\n",
        "\n",
        "for dataset_name in svox_datasets:\n",
        "    print(f\"\\n{'#'*60}\\nüåü INIZIO ELABORAZIONE DATASET: {dataset_name.upper()}\\n{'#'*60}\")\n",
        "\n",
        "    # Cartelle su Drive per il salvataggio permanente\n",
        "    drive_base = f\"/content/drive/MyDrive/VPR_Project_Final_Results/{dataset_name}\"\n",
        "    torch_drive_dir = f\"{drive_base}/torch_files\"\n",
        "    os.makedirs(torch_drive_dir, exist_ok=True)\n",
        "\n",
        "    for matcher in matchers:\n",
        "        print(f\"\\n{'-'*50}\\nüî• MATCHING CORRENTE: {matcher.upper()} ({dataset_name})\\n{'-'*50}\")\n",
        "\n",
        "        preds_dir = f\"logs/netvlad_{dataset_name}/predictions_txt\"\n",
        "        inliers_local_dir = f\"{preds_dir}_{matcher}\"\n",
        "\n",
        "        # A. Esecuzione Matching (512x512, top 20)\n",
        "        !python match_queries_preds.py \\\n",
        "            --preds-dir \"$preds_dir\" \\\n",
        "            --matcher \"$matcher\" \\\n",
        "            --device cuda \\\n",
        "            --im-size 512 \\\n",
        "            --num-preds 20\n",
        "\n",
        "        # B. Backup dei .torch su Drive\n",
        "        print(f\"üì¶ Backup file .torch su Drive...\")\n",
        "        dest_torch = f\"{torch_drive_dir}/{matcher}\"\n",
        "        if os.path.exists(dest_torch):\n",
        "            shutil.rmtree(dest_torch)\n",
        "        shutil.copytree(inliers_local_dir, dest_torch)\n",
        "        print(f\"‚úÖ Backup completato in: {dest_torch}\")\n",
        "\n",
        "        # C. Esecuzione Reranking (Recall base)\n",
        "        print(f\"üìà Calcolo Recall base per {matcher}...\")\n",
        "        !python reranking.py \\\n",
        "            --preds-dir \"$preds_dir\" \\\n",
        "            --inliers-dir \"$inliers_local_dir\" \\\n",
        "            --num-preds 20\n",
        "\n",
        "print(f\"\\n{'#'*60}\\nüéâ TUTTI I DATASET SVOX SONO COMPLETATI!\\n{'#'*60}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-hVXCewXrve",
        "outputId": "8b65689f-5b2f-4781-985f-0137559496b0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Visual-Place-Recognition-Project\n",
            "\n",
            "############################################################\n",
            "üåü INIZIO ELABORAZIONE DATASET: SVOX_SUN\n",
            "############################################################\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: SUPERPOINT-LG (svox_sun)\n",
            "--------------------------------------------------\n",
            "100% 712/712 [42:36<00:00,  3.59s/it]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "‚úÖ Backup completato in: /content/drive/MyDrive/VPR_Project_Final_Results/svox_sun/torch_files/superpoint-lg\n",
            "üìà Calcolo Recall base per superpoint-lg...\n",
            "100% 712/712 [00:05<00:00, 125.09it/s]\n",
            "R@1: 47.8, R@5: 49.6, R@10: 50.6, R@20: 51.4, R@100: 51.4\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: LOFTR (svox_sun)\n",
            "--------------------------------------------------\n",
            "100% 712/712 [44:11<00:00,  3.72s/it]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "‚úÖ Backup completato in: /content/drive/MyDrive/VPR_Project_Final_Results/svox_sun/torch_files/loftr\n",
            "üìà Calcolo Recall base per loftr...\n",
            "100% 712/712 [00:01<00:00, 658.16it/s]\n",
            "R@1: 48.9, R@5: 50.1, R@10: 50.8, R@20: 51.4, R@100: 51.4\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: SUPERGLUE (svox_sun)\n",
            "--------------------------------------------------\n",
            "Loaded SuperGlue model (\"outdoor\" weights)\n",
            "Loaded SuperPoint model\n",
            "Initialize SuperPoint_r4\n",
            "Initialize SuperGlue_r4\n",
            "100% 712/712 [17:34<00:00,  1.48s/it]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "‚úÖ Backup completato in: /content/drive/MyDrive/VPR_Project_Final_Results/svox_sun/torch_files/superglue\n",
            "üìà Calcolo Recall base per superglue...\n",
            "100% 712/712 [00:02<00:00, 315.18it/s]\n",
            "R@1: 46.8, R@5: 49.7, R@10: 50.7, R@20: 51.4, R@100: 51.4\n",
            "\n",
            "############################################################\n",
            "üåü INIZIO ELABORAZIONE DATASET: SVOX_NIGHT\n",
            "############################################################\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: SUPERPOINT-LG (svox_night)\n",
            "--------------------------------------------------\n",
            "100% 702/702 [41:48<00:00,  3.57s/it]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "‚úÖ Backup completato in: /content/drive/MyDrive/VPR_Project_Final_Results/svox_night/torch_files/superpoint-lg\n",
            "üìà Calcolo Recall base per superpoint-lg...\n",
            "100% 702/702 [00:05<00:00, 131.33it/s]\n",
            "R@1: 13.7, R@5: 15.5, R@10: 16.4, R@20: 17.8, R@100: 17.8\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: LOFTR (svox_night)\n",
            "--------------------------------------------------\n",
            "100% 702/702 [43:23<00:00,  3.71s/it]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "‚úÖ Backup completato in: /content/drive/MyDrive/VPR_Project_Final_Results/svox_night/torch_files/loftr\n",
            "üìà Calcolo Recall base per loftr...\n",
            "100% 702/702 [00:01<00:00, 695.10it/s]\n",
            "R@1: 13.7, R@5: 15.2, R@10: 15.8, R@20: 17.8, R@100: 17.8\n",
            "\n",
            "--------------------------------------------------\n",
            "üî• MATCHING CORRENTE: SUPERGLUE (svox_night)\n",
            "--------------------------------------------------\n",
            "Loaded SuperGlue model (\"outdoor\" weights)\n",
            "Loaded SuperPoint model\n",
            "Initialize SuperPoint_r4\n",
            "Initialize SuperGlue_r4\n",
            "100% 702/702 [17:11<00:00,  1.47s/it]\n",
            "üì¶ Backup file .torch su Drive...\n",
            "‚úÖ Backup completato in: /content/drive/MyDrive/VPR_Project_Final_Results/svox_night/torch_files/superglue\n",
            "üìà Calcolo Recall base per superglue...\n",
            "100% 702/702 [00:01<00:00, 474.84it/s]\n",
            "R@1: 13.2, R@5: 15.2, R@10: 16.5, R@20: 17.8, R@100: 17.8\n",
            "\n",
            "############################################################\n",
            "üéâ TUTTI I DATASET SVOX SONO COMPLETATI!\n",
            "############################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generazione File CSV"
      ],
      "metadata": {
        "id": "NSFd5YhjZjIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from util import get_list_distances_from_preds\n",
        "\n",
        "# --- 1. CONFIGURAZIONE ---\n",
        "# Specifichiamo i due sottoset di SVOX\n",
        "datasets = [\"svox_sun\", \"svox_night\"]\n",
        "matchers = [\"superpoint-lg\", \"loftr\", \"superglue\"]\n",
        "threshold = 25  # Soglia di 25 metri richiesta dal progetto\n",
        "\n",
        "def generate_svox_final_csvs():\n",
        "    for dataset in datasets:\n",
        "        print(f\"\\n{'='*60}\\nüìä GENERAZIONE CSV PER DATASET: {dataset.upper()}\\n{'='*60}\")\n",
        "\n",
        "        # Percorsi sul Drive (dove sono stati salvati i .torch nel passo precedente)\n",
        "        drive_base = f\"/content/drive/MyDrive/VPR_Project_Final_Results/{dataset}\"\n",
        "        csv_drive_dir = f\"{drive_base}/CSVs\"\n",
        "        torch_drive_dir = f\"{drive_base}/torch_files\"\n",
        "\n",
        "        # Percorso dei TXT locali (necessari per calcolare is_correct tramite distanze GPS)\n",
        "        preds_txt_dir = f\"/content/Visual-Place-Recognition-Project/logs/netvlad_{dataset}/predictions_txt\"\n",
        "\n",
        "        os.makedirs(csv_drive_dir, exist_ok=True)\n",
        "\n",
        "        for matcher in matchers:\n",
        "            matcher_torch_path = Path(f\"{torch_drive_dir}/{matcher}\")\n",
        "\n",
        "            if not matcher_torch_path.exists():\n",
        "                print(f\"‚ö†Ô∏è Cartella torch non trovata per {matcher} in {dataset}. Salto...\")\n",
        "                continue\n",
        "\n",
        "            print(f\"üîÑ Elaborazione {matcher}...\")\n",
        "            data_list = []\n",
        "\n",
        "            # Recuperiamo tutti i file .torch dal Drive\n",
        "            torch_files = sorted(list(matcher_torch_path.glob(\"*.torch\")), key=lambda x: int(x.stem))\n",
        "\n",
        "            for t_file in torch_files:\n",
        "                # 1. Recupero distanze reali (Ground Truth) dal file TXT corrispondente\n",
        "                txt_file = f\"{preds_txt_dir}/{t_file.stem}.txt\"\n",
        "                if not os.path.exists(txt_file):\n",
        "                    continue\n",
        "                geo_dists = get_list_distances_from_preds(txt_file)\n",
        "\n",
        "                # 2. Carico i tensori degli inliers dal Drive\n",
        "                query_results = torch.load(t_file, weights_only=False)\n",
        "\n",
        "                # 3. Estraggo il numero di inliers per i 20 candidati\n",
        "                inliers_counts = [res['num_inliers'] for res in query_results]\n",
        "\n",
        "                # 4. Trovo il massimo numero di inliers (confidenza della predizione)\n",
        "                max_val = max(inliers_counts) if inliers_counts else 0\n",
        "\n",
        "                # 5. Determino se il miglior match dopo il re-ranking √® corretto\n",
        "                best_idx = inliers_counts.index(max_val) if inliers_counts else 0\n",
        "                is_correct = 1 if (geo_dists[best_idx] <= threshold) else 0\n",
        "\n",
        "                data_list.append({\n",
        "                    'query_id': t_file.stem,\n",
        "                    'max_inliers': max_val,\n",
        "                    'is_correct': is_correct\n",
        "                })\n",
        "\n",
        "            # Creazione del DataFrame e salvataggio permanente su Drive\n",
        "            df = pd.DataFrame(data_list)\n",
        "            csv_name = f\"{matcher}_stats_final.csv\"\n",
        "            csv_path = f\"{csv_drive_dir}/{csv_name}\"\n",
        "            df.to_csv(csv_path, index=False)\n",
        "            print(f\"‚úÖ CSV Statistiche salvato: {csv_path} ({len(df)} query)\")\n",
        "\n",
        "# Esecuzione dello script di arricchimento\n",
        "generate_svox_final_csvs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gT_ytmQZmiI",
        "outputId": "7d77f497-37d8-4b1c-e711-71b8321bd1bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìä GENERAZIONE CSV PER DATASET: SVOX_SUN\n",
            "============================================================\n",
            "üîÑ Elaborazione superpoint-lg...\n",
            "‚úÖ CSV Statistiche salvato: /content/drive/MyDrive/VPR_Project_Final_Results/svox_sun/CSVs/superpoint-lg_stats_final.csv (712 query)\n",
            "üîÑ Elaborazione loftr...\n",
            "‚úÖ CSV Statistiche salvato: /content/drive/MyDrive/VPR_Project_Final_Results/svox_sun/CSVs/loftr_stats_final.csv (712 query)\n",
            "üîÑ Elaborazione superglue...\n",
            "‚úÖ CSV Statistiche salvato: /content/drive/MyDrive/VPR_Project_Final_Results/svox_sun/CSVs/superglue_stats_final.csv (712 query)\n",
            "\n",
            "============================================================\n",
            "üìä GENERAZIONE CSV PER DATASET: SVOX_NIGHT\n",
            "============================================================\n",
            "üîÑ Elaborazione superpoint-lg...\n",
            "‚úÖ CSV Statistiche salvato: /content/drive/MyDrive/VPR_Project_Final_Results/svox_night/CSVs/superpoint-lg_stats_final.csv (702 query)\n",
            "üîÑ Elaborazione loftr...\n",
            "‚úÖ CSV Statistiche salvato: /content/drive/MyDrive/VPR_Project_Final_Results/svox_night/CSVs/loftr_stats_final.csv (702 query)\n",
            "üîÑ Elaborazione superglue...\n",
            "‚úÖ CSV Statistiche salvato: /content/drive/MyDrive/VPR_Project_Final_Results/svox_night/CSVs/superglue_stats_final.csv (702 query)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisi delle Performance in Visual Place Recognition: Matching Geometrico, Strategie Adattive e Stima dell'Incertezza"
      ],
      "metadata": {
        "id": "9gY2cDBwH0-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# --- CONFIGURAZIONE PERCORSI ---\n",
        "drive_root = \"/content/drive/MyDrive/VPR_Project_Final_Results\"\n",
        "output_dir = \"final_plots\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Lista dei dataset e matcher che abbiamo elaborato\n",
        "datasets = [\"sf_xs\", \"tokyo_xs\", \"svox_sun\", \"svox_night\"]\n",
        "matchers = [\"superpoint-lg\", \"loftr\", \"superglue\"]\n",
        "\n",
        "def analyze_vpr_performance(csv_path, dataset_name, matcher_name):\n",
        "    if not os.path.exists(csv_path):\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    inlier_col = 'max_inliers'\n",
        "    correct_col = 'is_correct'\n",
        "\n",
        "    print(f\"üìà Analisi in corso: {dataset_name} + {matcher_name}\")\n",
        "\n",
        "    # --- 1. PLOT 6.1: ADAPTIVE TRADE-OFF (Recall vs Cost Saving) ---\n",
        "    # Il Cost Saving qui √® calcolato come quante query NON avrebbero bisogno\n",
        "    # di re-ranking se usassimo la soglia tau.\n",
        "    max_val = int(df[inlier_col].max())\n",
        "    thresholds = np.arange(0, max_val + 1, 1)\n",
        "    recalls = []\n",
        "    cost_savings = []\n",
        "    total_q = len(df)\n",
        "\n",
        "    for t in thresholds:\n",
        "        # Se inliers >= t, consideriamo la query \"EASY\" (niente re-ranking necessario)\n",
        "        # Se inliers < t, facciamo re-ranking.\n",
        "        # Qui simuliamo l'impatto sulla Recall finale.\n",
        "        correct_above_t = df[(df[correct_col] == 1) & (df[inlier_col] >= t)].shape[0]\n",
        "        recalls.append((correct_above_t / total_q) * 100)\n",
        "\n",
        "        # Pi√π alta √® la soglia, meno query consideriamo \"EASY\", meno risparmio abbiamo\n",
        "        easy_queries = df[df[inlier_col] >= t].shape[0]\n",
        "        cost_savings.append((easy_queries / total_q) * 100)\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "    ax1.plot(thresholds, recalls, color='tab:blue', label='Recall@1 (Adaptive)', linewidth=2)\n",
        "    ax1.set_xlabel('Inlier Threshold (œÑ)')\n",
        "    ax1.set_ylabel('Recall@1 (%)', color='tab:blue')\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(thresholds, cost_savings, color='tab:green', linestyle='--', label='Cost Saving (%)')\n",
        "    ax2.set_ylabel('Potential Computational Saving (%)', color='tab:green')\n",
        "    plt.title(f'6.1 Adaptive Re-ranking: {dataset_name} ({matcher_name})')\n",
        "    plt.savefig(f\"{output_dir}/{dataset_name}_{matcher_name}_6.1_adaptive.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # --- 2. PLOT 6.2: UNCERTAINTY (PR Curve per AUPRC) ---\n",
        "    # L'obiettivo √® vedere se \"max_inliers\" predice bene se una query √® sbagliata (is_correct=0)\n",
        "    # Invertiamo gli inliers perch√© meno inliers = pi√π probabilit√† di errore (incertezza)\n",
        "    uncertainty_score = -df[inlier_col]\n",
        "    y_true = 1 - df[correct_col] # 1 indica \"Errore\" (classe positiva per l'incertezza)\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_true, uncertainty_score)\n",
        "    auprc_score = auc(recall, precision)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='darkorange', label=f'PR Curve (AUPRC = {auprc_score:.3f})')\n",
        "    plt.xlabel('Recall (Detection of Errors)')\n",
        "    plt.ylabel('Precision (Confidence in Error Prediction)')\n",
        "    plt.title(f'6.2 Uncertainty Estimation: {dataset_name} ({matcher_name})')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.savefig(f\"{output_dir}/{dataset_name}_{matcher_name}_6.2_auprc.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return auprc_score\n",
        "\n",
        "# Esecuzione per tutti i file trovati su Drive\n",
        "summary_results = []\n",
        "for ds in datasets:\n",
        "    for mt in matchers:\n",
        "        csv_p = f\"{drive_root}/{ds}/CSVs/{mt}_stats_final.csv\"\n",
        "        score = analyze_vpr_performance(csv_p, ds, mt)\n",
        "        if score:\n",
        "            summary_results.append({'Dataset': ds, 'Matcher': mt, 'AUPRC': score})\n",
        "\n",
        "# --- 3. TABELLA RIASSUNTIVA ---\n",
        "df_summary = pd.DataFrame(summary_results)\n",
        "print(\"\\n--- RISULTATI FINALI PER REPORT ---\")\n",
        "print(df_summary)\n",
        "df_summary.to_csv(f\"{output_dir}/summary_metrics.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na2-Jn2zIAeD",
        "outputId": "41f42752-815c-417c-d243-bc8126c49141"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìà Analisi in corso: sf_xs + superpoint-lg\n",
            "üìà Analisi in corso: sf_xs + loftr\n",
            "üìà Analisi in corso: sf_xs + superglue\n",
            "üìà Analisi in corso: tokyo_xs + superpoint-lg\n",
            "üìà Analisi in corso: tokyo_xs + loftr\n",
            "üìà Analisi in corso: tokyo_xs + superglue\n",
            "üìà Analisi in corso: svox_sun + superpoint-lg\n",
            "üìà Analisi in corso: svox_sun + loftr\n",
            "üìà Analisi in corso: svox_sun + superglue\n",
            "üìà Analisi in corso: svox_night + superpoint-lg\n",
            "üìà Analisi in corso: svox_night + loftr\n",
            "üìà Analisi in corso: svox_night + superglue\n",
            "\n",
            "--- RISULTATI FINALI PER REPORT ---\n",
            "       Dataset        Matcher     AUPRC\n",
            "0        sf_xs  superpoint-lg  0.958964\n",
            "1        sf_xs          loftr  0.955359\n",
            "2        sf_xs      superglue  0.955096\n",
            "3     tokyo_xs  superpoint-lg  0.966857\n",
            "4     tokyo_xs          loftr  0.960619\n",
            "5     tokyo_xs      superglue  0.919419\n",
            "6     svox_sun  superpoint-lg  0.978083\n",
            "7     svox_sun          loftr  0.977251\n",
            "8     svox_sun      superglue  0.974188\n",
            "9   svox_night  superpoint-lg  0.993302\n",
            "10  svox_night          loftr  0.978218\n",
            "11  svox_night      superglue  0.986273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zip e download cartella final_plot"
      ],
      "metadata": {
        "id": "RX5xJ9l4Jgym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r final_plots.zip final_plots/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaK1uFGLJNto",
        "outputId": "78d46f4d-ec15-4870-f145-d3fd439d0969"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: final_plots/ (stored 0%)\n",
            "  adding: final_plots/svox_night_superpoint-lg_6.2_auprc.png (deflated 15%)\n",
            "  adding: final_plots/tokyo_xs_superpoint-lg_6.1_adaptive.png (deflated 6%)\n",
            "  adding: final_plots/sf_xs_superglue_6.1_adaptive.png (deflated 7%)\n",
            "  adding: final_plots/svox_night_loftr_6.1_adaptive.png (deflated 7%)\n",
            "  adding: final_plots/svox_sun_loftr_6.1_adaptive.png (deflated 6%)\n",
            "  adding: final_plots/svox_sun_superpoint-lg_6.2_auprc.png (deflated 15%)\n",
            "  adding: final_plots/sf_xs_loftr_6.2_auprc.png (deflated 14%)\n",
            "  adding: final_plots/svox_night_superglue_6.1_adaptive.png (deflated 6%)\n",
            "  adding: final_plots/svox_night_superglue_6.2_auprc.png (deflated 15%)\n",
            "  adding: final_plots/sf_xs_superpoint-lg_6.1_adaptive.png (deflated 6%)\n",
            "  adding: final_plots/tokyo_xs_loftr_6.1_adaptive.png (deflated 6%)\n",
            "  adding: final_plots/svox_sun_superpoint-lg_6.1_adaptive.png (deflated 6%)\n",
            "  adding: final_plots/svox_sun_superglue_6.2_auprc.png (deflated 14%)\n",
            "  adding: final_plots/sf_xs_loftr_6.1_adaptive.png (deflated 6%)\n",
            "  adding: final_plots/tokyo_xs_loftr_6.2_auprc.png (deflated 16%)\n",
            "  adding: final_plots/sf_xs_superpoint-lg_6.2_auprc.png (deflated 14%)\n",
            "  adding: final_plots/summary_metrics.csv (deflated 53%)\n",
            "  adding: final_plots/svox_sun_loftr_6.2_auprc.png (deflated 16%)\n",
            "  adding: final_plots/tokyo_xs_superpoint-lg_6.2_auprc.png (deflated 16%)\n",
            "  adding: final_plots/tokyo_xs_superglue_6.2_auprc.png (deflated 14%)\n",
            "  adding: final_plots/tokyo_xs_superglue_6.1_adaptive.png (deflated 6%)\n",
            "  adding: final_plots/svox_night_superpoint-lg_6.1_adaptive.png (deflated 6%)\n",
            "  adding: final_plots/sf_xs_superglue_6.2_auprc.png (deflated 13%)\n",
            "  adding: final_plots/svox_sun_superglue_6.1_adaptive.png (deflated 7%)\n",
            "  adding: final_plots/svox_night_loftr_6.2_auprc.png (deflated 16%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('final_plots.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "b_-1AoLnJkk9",
        "outputId": "4160aa22-9468-4b82-ada8-5372e0f07e03"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_de8a3a6d-fd43-4f9e-b002-faf998e302be\", \"final_plots.zip\", 904162)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}